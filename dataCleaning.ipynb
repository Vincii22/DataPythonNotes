{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e87709",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_csv('sales.csv')\n",
    "sales.head(2)\n",
    "\n",
    "#get the datatype of columns\n",
    "\n",
    "sales.dtypes\n",
    "\n",
    "#get dataframe information\n",
    "sales.info()\n",
    "\n",
    "\n",
    "#Print sum of all revenue column\n",
    "sales['Revenue'].sum()\n",
    "\n",
    "sales['Revenue'] = sales['Revenue'].str.strip('$')\n",
    "sales['Revenue'] = sales['Revenue'].astype('int')\n",
    "\n",
    "#Assert is one way to verify something (like if) if it is True then it returns nothing\n",
    "\n",
    "assert 1+1 == 2\n",
    "\n",
    "#Otherwise (False) it returns an error\n",
    "assert 1+1 == 3\n",
    "assert sales['Revenue'].dtype == 'int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3cf5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical or Categorical?\n",
    "\n",
    "df['marriage_status'].describe()\n",
    "\n",
    "#Convert to categorical\n",
    "\n",
    "df['marriage_status'] = df['marriage_status'].astype('category')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbeca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercises\n",
    "\n",
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the information of ride_sharing\n",
    "print(ride_sharing.info())\n",
    "\n",
    "# Print summary statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())\n",
    "\n",
    "# Convert user_type from integer to category\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "# Write an assert statement confirming the change\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
    "\n",
    "# Print new summary statistics \n",
    "print(ride_sharing['user_type_cat'].describe())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2\n",
    "\n",
    "# Strip duration of minutes\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
    "\n",
    "# Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
    "\n",
    "# Write an assert statement making sure of conversion\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "# Print formed columns and calculate average ride duration \n",
    "print(ride_sharing[['duration','duration_trim','duration_time']])\n",
    "print(ride_sharing['duration_time'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc25f1",
   "metadata": {},
   "source": [
    "Data Range Constraints\n",
    "\n",
    "Motivation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(movies['avg_rating'])\n",
    "plt.title('Average rating of movies (1-5)')\n",
    "\n",
    "\n",
    "How to deal with out of range data?\n",
    "\n",
    "- Dropping Data\n",
    "- Setting custom minimums and maximums\n",
    "- Treat as missing and impute\n",
    "- Setting custom value depending on business assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d61768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "# Output movies with rating > 5\n",
    "movies[movies['avg_rating'] > 5]\n",
    "\n",
    "#Drop values using filtering example\n",
    "\n",
    "movies = movies[movies['avg_rating'] <= 5]\n",
    "\n",
    "movies.drop(movies[movies['avg_rating'] > 5].index, inplace = True)\n",
    "\n",
    "# Convert avg_rating > 5 to 5\n",
    "movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5\n",
    "\n",
    "assert movies['avg_rating'].max() <= 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2601f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data range examples\n",
    "\n",
    "import datetime as dt\n",
    "today_date = dt.date.today()\n",
    "user_signups[user_signups['subscribtion_date'] > dt.date.today()]\n",
    "\n",
    "import datetime as dt \n",
    "import pandas as pd\n",
    "\n",
    "#Outputs data types\n",
    "user_signups.dtypes\n",
    "\n",
    "#Convert to date\n",
    "\n",
    "user_signups['subscription_date'] = pd.to_datetime(user_signups['subscription_date']).dt.date\n",
    "\n",
    "\n",
    "today_date = dt.date.today()\n",
    "\n",
    "# Drop Data \n",
    "\n",
    "#Drop values using filtering\n",
    "user_signups = user_signups[user_signups['subscription_date'] < today_date]\n",
    "#Drop values using .drop()\n",
    "user_signups.drop(user_signups[user_signups['subscription_date'] > today_date].index, inplace = True)\n",
    "\n",
    "\n",
    "#Hardcode dates with upper limit\n",
    "\n",
    "user_signups.loc[user_signups['subscription_date'] > today_date, 'subscription_date'] = today_date \n",
    "assert user_signups.subscription_date.max().date() <= today_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfc4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercises\n",
    "\n",
    "# Convert tire_sizes to integer\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "# Set all values above 27 to 27\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "\n",
    "# Reconvert tire_sizes back to categorical\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "# Print tire size description\n",
    "print(ride_sharing['tire_sizes'].describe())\n",
    "\n",
    "\n",
    "# Convert ride_date to date\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date']).dt.date\n",
    "\n",
    "# Save today's date\n",
    "today = dt.date.today()\n",
    "\n",
    "# Set all in the future to today's date\n",
    "ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today\n",
    "\n",
    "# Print maximum of ride_dt column\n",
    "print(ride_sharing['ride_dt'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a3359a",
   "metadata": {},
   "source": [
    "Uniqueness Constraints\n",
    "\n",
    "Duplicate Values\n",
    "\n",
    "- Most columns have the same values\n",
    "\n",
    "Why do they happen?\n",
    "\n",
    "- Data Entry & Human Error\n",
    "- Bugs and design error\n",
    "- Join or merge errors\n",
    "\n",
    "How to find duplicate rows?\n",
    "\n",
    "- The .duplicated() method\n",
    "- subset: List of columns names to check for duplication\n",
    "- keep: Whether to keep first('first'), last('last'), or all (False) duplicate values\n",
    "\n",
    "- .sort_values\n",
    "\n",
    "\n",
    "How to treat duplicated values?\n",
    "\n",
    "- The .drop_duplicates method\n",
    "- subset: List of columns to check for duplication\n",
    "- keep: Whether to keep first('first'), last('last'), or all (False) duplicate values\n",
    "- inplace: Drop duplicated rows directly inside DataFrame without creating new object (True).\n",
    "\n",
    "- The .groupby() and agg() methods\n",
    "- .reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7753b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_weight.head()\n",
    "\n",
    "# Get duplicates across columns\n",
    "\n",
    "duplicates = height_weight.duplicated() #Duplicated Keyword (Method)\n",
    "print(duplicates)\n",
    "\n",
    "# Get duplicate rows\n",
    "\n",
    "height_weight[duplicates]\n",
    "\n",
    "\n",
    "\n",
    "#Column names to check for duplication\n",
    "\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "duplicates = height_weight.duplicated(subset = column_names, keep = False)\n",
    "\n",
    "height_weight[duplicates]\n",
    "\n",
    "#Output duplicated values\n",
    "height_weight[duplicates].sort_values(by = 'first_name')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f37356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hoow to treat duplicated values\n",
    "\n",
    "#Drop duplicates\n",
    "\n",
    "height_weight.drop_duplicates(inplace = True)\n",
    "\n",
    "#Output duplicated values\n",
    "\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "duplicates = height_weight.duplicated(subset = column_names, keep = False)\n",
    "height_weight[duplicates],sort_values(by = 'first_name')\n",
    "\n",
    "\n",
    "# Group by colum names and produce statistical summaries\n",
    "\n",
    "column_names = ['first_name', 'last_name', 'address']\n",
    "summaries = {'height': 'max', 'weight': 'mean'}\n",
    "height_weight = height_weight.groupby(by = column_names).agg(summaries).reset_index()\n",
    "\n",
    "# Make sure the aggregation is done\n",
    "\n",
    "duplicates = height_weight.duplicated(subset = column_names, keep = False)\n",
    "height_weight[duplicates].sort_values(by = 'first_name')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples\n",
    "\n",
    "# Find duplicated rows of ride_id in the ride_sharing DataFrame while setting keep to False.\n",
    "# Subset ride_sharing on duplicates and sort by ride_id and assign the results to duplicated_rides.\n",
    "# Print the ride_id, duration and user_birth_year columns of duplicated_rides in that order\n",
    "\n",
    "# Find duplicates\n",
    "duplicates = ride_sharing.duplicated(subset = 'ride_id', keep = False)\n",
    "\n",
    "# Sort your duplicated rides\n",
    "duplicated_rides = ride_sharing[duplicates].sort_values('ride_id')\n",
    "\n",
    "# Print relevant columns of duplicated_rides\n",
    "print(duplicated_rides[['ride_id','duration','user_birth_year']])\n",
    "\n",
    "\n",
    "#Instruction\n",
    "\n",
    "# Drop complete duplicates in ride_sharing and store the results in ride_dup.\n",
    "# Create the statistics dictionary which holds minimum aggregation for user_birth_year and mean aggregation for duration.\n",
    "# Drop incomplete duplicates by grouping by ride_id and applying the aggregation in statistics.\n",
    "# Find duplicates again and run the assert statement to verify de-duplication.\n",
    "\n",
    "\n",
    "# Drop complete duplicates from ride_sharing\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create statistics dictionary for aggregation function\n",
    "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
    "\n",
    "# Group by ride_id and compute new statistics\n",
    "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
    "\n",
    "# Find duplicated values again\n",
    "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
    "duplicated_rides = ride_unique[duplicates == True]\n",
    "\n",
    "# Assert duplicates are processed\n",
    "assert duplicated_rides.shape[0] == 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e4064",
   "metadata": {},
   "source": [
    "Membership Constraints\n",
    "\n",
    "Categories and Membership Constraints\n",
    "\n",
    "Predefined finite sets of categories\n",
    "\n",
    "Type of Data                    Example Values              Numeric Representation\n",
    "\n",
    "Marriage Status                 unmarried, married              0,1\n",
    "Household income category       0-20k, 20-40k                   0,1, ...\n",
    "Loand Status                    default, paid, no_loan          0, 1, 2\n",
    "\n",
    "\n",
    "Why could we have these problems?\n",
    "\n",
    "- Data Entry Errors\n",
    "- Parsing Errors\n",
    "\n",
    "How do we treat these problems?\n",
    "\n",
    "- Dropping data\n",
    "- Remapping categories\n",
    "- Inferring categories\n",
    "\n",
    "- categories\n",
    "- Methods:\n",
    "- Using set()\n",
    "- difference()\n",
    "- isin()\n",
    "\n",
    "- ~ (negate)\n",
    "A note on Joins\n",
    "\n",
    "Anti Joins                                      Inner Joins\n",
    "\n",
    "A   B                                            A       B\n",
    "\n",
    "What is in A and not in B                    What is in both A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read study data and print it \n",
    "\n",
    "study_data = pd.read_csv('study.csv')\n",
    "study_data\n",
    "\n",
    "#correct possible blood_types\n",
    "\n",
    "categories\n",
    "\n",
    "#Finding inconsistent categories\n",
    "\n",
    "inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n",
    "print(inconsistent_categories)\n",
    "\n",
    "#Get and print rows with inconsistent categories\n",
    "inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)\n",
    "study_data[inconsistent_rows]\n",
    "\n",
    "\n",
    "#Dropping inconsistent categories\n",
    "\n",
    "inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])\n",
    "inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)\n",
    "inconsistent_data = study_data[inconsistent_rows]\n",
    "\n",
    "# Drop inconsistent categories and get consistent data only\n",
    "consistent_data = study_data[~inconsistent_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad28df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercises\n",
    "\n",
    "# Print categories DataFrame\n",
    "print(categories)\n",
    "\n",
    "# Print unique values of survey columns in airlines\n",
    "print('Cleanliness: ', airlines['cleanliness'].unique(), \"\\n\")\n",
    "print('Safety: ', airlines['safety'].unique(), \"\\n\")\n",
    "print('Satisfaction: ', airlines['satisfaction'].unique(), \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "\n",
    "#Instructions:\n",
    "\n",
    "# Create a set out of the cleanliness column in airlines using set() and find the inconsistent category by finding the difference in the cleanliness column of categories.\n",
    "# Find rows of airlines with a cleanliness value not in categories and print the output.\n",
    "\n",
    "# Find the cleanliness category in airlines not in categories\n",
    "cat_clean = set(airlines['cleanliness']).difference(categories['cleanliness'])\n",
    "\n",
    "# Find rows with that category\n",
    "cat_clean_rows = airlines['cleanliness'].isin(cat_clean)\n",
    "\n",
    "# Print rows with inconsistent category\n",
    "print(airlines[cat_clean_rows])\n",
    "\n",
    "# Print rows with consistent categories only\n",
    "print(airlines[~cat_clean_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5eaa3c",
   "metadata": {},
   "source": [
    "Categorical Variables\n",
    "\n",
    "What type of errors could we have?\n",
    "\n",
    "- Value inconsistency\n",
    "    - Inconsistent fields: 'married', 'Maried', 'UNMARRIED', 'not married'..\n",
    "    - _Trailing white spaces:_'married', ' married '...\n",
    "- Collapsing too many categories to few\n",
    "    - Creating new groups: 0-20K, 20-40K categories..from continuous household income data\n",
    "    - Mapping groups to new ones: Mapping household income categories to 2 'rich', 'poor'\n",
    "- Making sure data is of type category(seen in chapter 1)\n",
    "\n",
    "\n",
    "Value Consistency\n",
    "\n",
    "- Capitalization: 'married', 'Maried', 'UNMARRIED', 'not married'.. \n",
    "\n",
    "- Methods:\n",
    "- demographics[]\n",
    "- value_counts()\n",
    "- str_upper() - to Capitalize\n",
    "- str_lower() - to lowercase\n",
    "- str_strip() - to remove whitespaces\n",
    "\n",
    "Collapsing data into categories\n",
    "\n",
    "- Methods:\n",
    "- qcut() \n",
    "    - q, labels\n",
    "- cut()\n",
    "    - bins - ranges, labels - name, np.inf\n",
    "\n",
    "- Map categories to fewer ones: reducing categories in categorical column\n",
    "    - operating_system column is: 'Microsoft', 'MacOS', 'IOS', 'Android', 'Linux'\n",
    "    - operating_system column should become: 'DesktopOS', 'MobileOS'\n",
    "- Methods:\n",
    "- replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775edbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09feb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value Consistency\n",
    "#Get marriage status column\n",
    "marriage_status = demographics['marriage_status']\n",
    "marriage_status.value_counts()\n",
    "\n",
    "#get value counts on DataFrame\n",
    "marriage_status.groupby('marriage_status').count()\n",
    "\n",
    "#Capitalize\n",
    "marriage_status['marriage_status'] = marriage_status['marriage_status'].str_upper()\n",
    "marriage_status['marriage_status'].value_counts()\n",
    "\n",
    "#Lowercase\n",
    "marriage_status['marriage_status'] = marriage_status['marriage_status'].str_lower()\n",
    "marriage_status['marriage_status'].value_counts()\n",
    "\n",
    "# Get marriage status column\n",
    "marriage_status = demographics['marriage_status']\n",
    "marriage_status.value_counts()\n",
    "\n",
    "\n",
    "#Strip all spaces\n",
    "demographics = demographics['marriage_status'].str_strip()\n",
    "demographics[marriage_status].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd352cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collapsing data into categories\n",
    "#Using qcut\n",
    "import pandas as pd\n",
    "group_names = ['0-200K', '200K-500K', '500K+']\n",
    "demographics[income_group] = pd.qcut(demographics['household_income'], q = 3, labels = group_names)\n",
    "\n",
    "#Print income_group column\n",
    "demographics[['income_group', 'household_income']]\n",
    "\n",
    "\n",
    "#Using cut()\n",
    "ranges = [0, 20000, 50000, np.inf]\n",
    "group_names = ['0-200K', '200K-500K', '500K+']\n",
    "\n",
    "#Create income group column\n",
    "demographics['income_group'] = pd.cut(demographics['household_income'], bins = ranges, labels = group_names)\n",
    "demographics[['income_group', 'household_income']]\n",
    "\n",
    "\n",
    "# - Map categories to fewer ones: reducing categories in categorical column\n",
    "\n",
    "mapping = {'Microsoft': 'DesktopOS', 'MacOS': 'DesktopOS', 'Linux': 'DesktopOS', 'IOS': 'MobileOS', 'Android': 'MobileOS'}\n",
    "devices['operating_system'] = devices['operating_system'].replace(mapping)\n",
    "devices['operating_system'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercises\n",
    "\n",
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "\n",
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower()\n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "\n",
    "# Print unique values of both columns\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "# Lower dest_region column and then replace \"eur\" with \"europe\"\n",
    "airlines['dest_region'] = airlines['dest_region'].str.lower() \n",
    "airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "# Remove white spaces from `dest_size`\n",
    "airlines['dest_size'] = airlines['dest_size'].str.strip()\n",
    "\n",
    "# Verify changes have been effected\n",
    "print(airlines['dest_region'].unique())\n",
    "print(airlines['dest_size'].unique())\n",
    "\n",
    "\n",
    "#Instructions\n",
    "\n",
    "# Create the ranges and labels for the wait_type column mentioned in the description.\n",
    "# Create the wait_type column by from wait_min by using pd.cut(), while inputting label_ranges and label_names in the correct arguments.\n",
    "# Create the mapping dictionary mapping weekdays to 'weekday' and weekend days to 'weekend'.\n",
    "# Create the day_week column by using .replace().\n",
    "\n",
    "# Create ranges for categories\n",
    "label_ranges = [0, 60, 180, np.inf]\n",
    "label_names = ['short', 'medium', 'long']\n",
    "\n",
    "# Create wait_type column\n",
    "airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, \n",
    "                                labels = label_names)\n",
    "\n",
    "# Create mappings and replace\n",
    "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
    "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
    "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
    "\n",
    "airlines['day_week'] = airlines['day'].replace(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983efadb",
   "metadata": {},
   "source": [
    "Cleaning Text Data\n",
    "\n",
    "What is Text Data?\n",
    "\n",
    "Type of Data                Example Values\n",
    "Names                       Alex, Sara,...\n",
    "Phone Numbers               +96171679912 ...\n",
    "Emails                      `adel@datacamp.com`\n",
    "Passwords                   ...\n",
    "\n",
    "Common text data problems:\n",
    "\n",
    "- Data Inconsistency:\n",
    "    - +96171679912 or 0096171679912 or ..?\n",
    "- Fixed Lengths violations:\n",
    "    - Password needs to be atleast 8 characters\n",
    "- Typos:\n",
    "    - +961.71.679912\n",
    "\n",
    "- Methods | Keywords:\n",
    "    - np - numpy\n",
    "    - str.replace\n",
    "    - str.len\n",
    "    - loc[] - Label-based indexing\n",
    "    - str.contains()\n",
    "    - any()\n",
    "\n",
    "Regular Expression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abefdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "phones = pd.read_csv('phones.csv')\n",
    "print(phones)\n",
    "# Replace '+' with '00'\n",
    "phones['Phone Number'] = phones['Phone Number'].str.replace(\"+\", \"00\")\n",
    "\n",
    "# Replace phone numbers with lower than 10 digits to NaN\n",
    "digits = phones['Phone number'].str.len()\n",
    "phones.loc[digits < 10, 'Phone number'] = np.nan\n",
    "phones\n",
    "\n",
    "# Find length of each row in Phone number column\n",
    "sanity_check = phone['Phone number'].str.len()\n",
    "\n",
    "# Assert minimum phone number length is 10\n",
    "assert sanity_check.min() >= 10\n",
    "\n",
    "# Assert all numbers do not have \"+\" or \"-\"\n",
    "assert phone['Phone Number'].str.contains(\"+|-\").any() == False\n",
    "\n",
    "# Replace letters with nothing\n",
    "phones['Phone Number'] = phones['Phone Number'].str.replace(r'\\D+', '')\n",
    "phones.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises\n",
    "\n",
    "# Replace \"Dr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(r\"Dr.\",\"\")\n",
    "\n",
    "# Replace \"Mr.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(r\"Mr.\", \"\")\n",
    "\n",
    "# Replace \"Miss\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(r\"Miss\", \"\")\n",
    "\n",
    "# Replace \"Ms.\" with empty string \"\"\n",
    "airlines['full_name'] = airlines['full_name'].str.replace(r\"Ms.\", \"\")\n",
    "\n",
    "# Assert that full_name has no honorifics\n",
    "assert airlines['full_name'].str.contains('Ms.|Mr.|Miss|Dr.').any() == False\n",
    "\n",
    "\n",
    "#Instructions\n",
    "\n",
    "# Using the airlines DataFrame, store the length of each instance in the survey_response column in resp_length by using .str.len().\n",
    "# Isolate the rows of airlines with resp_length higher than 40.\n",
    "# Assert that the smallest survey_response length in airlines_survey is now bigger than 40.\n",
    "\n",
    "\n",
    "# Store length of each row in survey_response column\n",
    "resp_length = airlines['survey_response'].str.len()\n",
    "\n",
    "# Find rows in airlines where resp_length > 40\n",
    "airlines_survey = airlines[resp_length > 40]\n",
    "\n",
    "# Assert minimum survey_response length is > 40\n",
    "assert airlines_survey['survey_response'].str.len().min() > 40\n",
    "\n",
    "# Print new survey_response column\n",
    "print(airlines_survey['survey_response'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde385f2",
   "metadata": {},
   "source": [
    "Uniformity\n",
    "\n",
    "Column              Unit\n",
    "- Temperature       - 32 Degree Celsius is also 89.6 Farenheit\n",
    "- Weight            - 70 kg is also 11 st.\n",
    "- Date              - 26-11-2019 is also 26, November, 2019\n",
    "- Money             - 100$ is also 10763.90 Yen\n",
    "\n",
    "\n",
    "Datetime formatting\n",
    "\n",
    "datetime is useful for representing dates                   pandas.to_datetime()\n",
    "\n",
    "Date                         datetime format                - Can recognize most formats automatically\n",
    "25-12-2019                  %d-%m-%Y                        - Sometimes fails with erroneous or unrecognizable formats\n",
    "December 25th 2019          %c\n",
    "12-25-2019                  %m-%d-%Y\n",
    "\n",
    "Treating ambiguous date data\n",
    "\n",
    "- Is 2019-03-08 in August or March?\n",
    "- Convert to NA and treat accordingly\n",
    "- Infer format by understanding data source\n",
    "- Infer format by understanding previous and subsequent data in DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a5923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "temperatures = pd.read_csv('temperature.csv')\n",
    "temperatures.head()\n",
    "\n",
    "plt.scatter(x = 'Date', y = 'Temperature', data = temperatures)\n",
    "# Create title, xlabel, and ylabel\n",
    "plt.title('Temperature in Celsius March 2019 - NYC')\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Temperatures in Celsius')\n",
    "#Show plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "temp_fah = temperatures.loc[temperatures['Temperature'] > 40, ['Temperature']]\n",
    "temp_cels = (temp_fah - 32) * (5/9)\n",
    "temperatures.loc[temperatures['Temperature'] > 40, 'Temperature'] = temp_cels\n",
    "\n",
    "assert temperatures['Temperature'].max() < 40\n",
    "\n",
    "\n",
    "birhdays.head()\n",
    "\n",
    "# Converts to datetime - but won't work\n",
    "birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'])\n",
    "# Returns NA for rows where conversion failed\n",
    "birthdays['Birthday'] = pd.to_datetime(birthdays['Birthday'], errors = 'coerce')\n",
    "\n",
    "#Treating date data\n",
    "birthdays['Birthday'] = birthdays['Birthday'].dt.strftime(\"%d-%m-%Y\")\n",
    "birthdays.head()\n",
    "\n",
    "\n",
    "\n",
    "# Find values of acct_cur that are equal to 'euro'\n",
    "acct_eu = banking['acct_cur'] == 'euro'\n",
    "\n",
    "# Convert acct_amount where it is in euro to dollars\n",
    "banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1\n",
    "\n",
    "# Unify acct_cur column by changing 'euro' values to 'dollar'\n",
    "banking.loc[acct_eu, 'acct_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currency remains\n",
    "assert banking['acct_cur'].unique() == 'dollar'\n",
    "\n",
    "\n",
    "# Print the header of account_opened\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Print the header of account_opened\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "\n",
    "# Print the header of account_opend\n",
    "print(banking['account_opened'].head())\n",
    "\n",
    "# Convert account_opened to datetime\n",
    "banking['account_opened'] = pd.to_datetime(banking['account_opened'],\n",
    "                                           # Return missing value for error\n",
    "                                           errors = 'coerce') \n",
    "\n",
    "# Get year of account opened\n",
    "banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "# Print acct_year\n",
    "print(banking['acct_year'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f4945",
   "metadata": {},
   "source": [
    "Cross Field Validation\n",
    "\n",
    "- The use of multiple fields in a dataset to sanity check data integrity\n",
    "\n",
    "\n",
    "What to do when we catch inconsistencies?\n",
    "\n",
    "- Dropping data\n",
    "- Set to missing and impute \n",
    "- Apply rules from domain knowledge\n",
    "\n",
    "- Methods:\n",
    "    - sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037efc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Field Validation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "flights = pd.read_csv('flights.csv')\n",
    "flights.head()\n",
    "\n",
    "sum_classes = flights[['economy_class', 'business_class', 'first_class']].sum(axis = 1)\n",
    "passenger_equ = sum_classes == flights['total_passengers']\n",
    "# Find and filter rows with inconsistent passenger totals\n",
    "inconsistent_pass = flights[~passenger_equ]\n",
    "consistent_pass = flights[passenger_equ]\n",
    "\n",
    "\n",
    "\n",
    "users.head()\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# Convert to datetime to get today's date\n",
    "users['Birthday'] = pd.to_datetime(users['Birthday'])\n",
    "today = dt.date.today()\n",
    "\n",
    "# For each row in the Birthday column, calculate year difference\n",
    "age_manual = today.year - users['Birthday'].dt.year\n",
    "# Find instances where ages match\n",
    "age_equ = age_manual == users['Age']\n",
    "\n",
    "# Find and filter rows with inconsistent age\n",
    "inconsistent_age = users[~age_equ]\n",
    "consistent_age = users[age_equ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e77f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Field Validation Examples\n",
    "\n",
    "# Store fund columns to sum against\n",
    "fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "# Find rows where fund_columns row sum == inv_amount\n",
    "inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_inv = banking[inv_equ]\n",
    "inconsistent_inv = banking[~inv_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent investments: \", inconsistent_inv.shape[0])\n",
    "\n",
    "\n",
    "# Store today's date and find ages\n",
    "today = dt.date.today()\n",
    "ages_manual = today.year - banking['birth_date'].dt.year\n",
    "\n",
    "# Find rows where age column == ages_manual\n",
    "age_equ = banking['age'] == ages_manual\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "consistent_ages = banking[age_equ]\n",
    "inconsistent_ages = banking[~age_equ]\n",
    "\n",
    "# Store consistent and inconsistent data\n",
    "print(\"Number of inconsistent ages: \", inconsistent_ages.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc84b46",
   "metadata": {},
   "source": [
    "Completeness\n",
    "\n",
    "- What is missing data?\n",
    "- Occurs when no data value is stored for a variable in an observation\n",
    "- Can be represented as NA, NaN, 0, . ...\n",
    "- Technical Error\n",
    "- Human Error\n",
    "\n",
    "Missingness types\n",
    "\n",
    "- Missing completely at random (MCAR)\n",
    "    No systematic relationship between missing data and other values\n",
    "    Data entry errors when inputting data\n",
    "- Missing at random  (MAR)\n",
    "    Systematic relationship between missing data and other observed values\n",
    "    Missing ozone data for high temperatures\n",
    "- Missing not at random (MNAR)\n",
    "    Systematic relationship between missing data and unobserved values\n",
    "    Missing temperatures values for high temperatures\n",
    "\n",
    "How to deal with missing data?\n",
    "Simple approaches\n",
    "- Drop missing data\n",
    "- Impute with statistical measures (mean, median, mode)\n",
    "\n",
    "More complex approaches\n",
    "- Imputting using an algorithmic approach\n",
    "- Impute with machine learning models\n",
    "\n",
    "Missingno\n",
    "- Useful package for visualizing and understanding data\n",
    "- Packages:\n",
    "    import missingno as msno\n",
    "    import matplotlib.pyplot as plt\n",
    "- Methods:\n",
    "    - .isna()\n",
    "    - .dropna(subset)\n",
    "    - .fillna(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f827b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Air quality example\n",
    "\n",
    "import pandas as pd\n",
    "airquality = pd.read_csv('airquality.csv')\n",
    "print(airquality)\n",
    "\n",
    "# Return missing values\n",
    "\n",
    "airquality.isna()\n",
    "\n",
    "# Get summary of missingness\n",
    "\n",
    "airquality.isna().sum()\n",
    "\n",
    "\n",
    "\n",
    "# Visualize missingness\n",
    "msno.matrix(airquality)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and complete values aside\n",
    "\n",
    "missing = airquality[airquality['CO2'].isna()]\n",
    "complete = airquality[~airquality['CO2'].isna()]\n",
    "\n",
    "# Describe complete DataFrame\n",
    "complete.describe()\n",
    "\n",
    "# Describe missing DataFrame\n",
    "missing.describe()\n",
    "\n",
    "sorted_airquality = airquality.sort_values(by = 'Temperature')\n",
    "msno.matrix(sorted_airquality)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Dealing with missing data\n",
    "\n",
    "airquality_dropped = airquality.dropna(subset = ['CO2'])\n",
    "airquality_dropped.head()\n",
    "\n",
    "# Replacing with statistical measures\n",
    "co2_mean = airquality['CO2'].mean()\n",
    "airquality_imputed = airquality.fillna({'CO2': co2_mean})\n",
    "airquality_imputed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examples\n",
    "\n",
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]\n",
    "\n",
    "\n",
    "\n",
    "# Print number of missing values in banking\n",
    "print(banking.isna().sum())\n",
    "\n",
    "# Visualize missingness matrix\n",
    "msno.matrix(banking)\n",
    "plt.show()\n",
    "\n",
    "# Isolate missing and non missing values of inv_amount\n",
    "missing_investors = banking[banking['inv_amount'].isna()]\n",
    "investors = banking[~banking['inv_amount'].isna()]\n",
    "\n",
    "# Sort banking by age and visualize\n",
    "banking_sorted = banking.sort_values(by = 'age')\n",
    "msno.matrix(banking_sorted)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Drop missing values of cust_id\n",
    "banking_fullid = banking.dropna(subset = ['cust_id'])\n",
    "\n",
    "# Compute estimated acct_amount\n",
    "acct_imp = banking_fullid['inv_amount'] * 5\n",
    "\n",
    "# Impute missing acct_amount with corresponding acct_imp\n",
    "banking_imputed = banking_fullid.fillna({'acct_amount': acct_imp})\n",
    "\n",
    "# Print number of missing values\n",
    "print(banking_imputed.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d41da9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
